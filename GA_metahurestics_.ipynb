{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cunn7iZxdAw",
        "outputId": "89358a92-5a23-4e7c-f573-0cccd06526c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hOTMzoRBdiyv"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/cs_668')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Fsh_IKVi_p",
        "outputId": "b20570b2-8ac7-4363-d38f-1285b0486ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/cs_668\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xca39p_Gxeax"
      },
      "source": [
        "Dataets:\n",
        "  * From LR : iris , from sklearn.datasets\n",
        "  * From LR :  wine,   from sklearn.datasets\n",
        "  * From LR: Concrete Compressive Strength\n",
        " https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength , uploaded in drive\n",
        "\n",
        "  * From LR : Breast Cancer Wisconsin : https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/data , uploaded in drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8SENA2fnQX81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris,load_wine\n",
        "import random\n",
        "from collections import deque\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import math\n",
        "from sklearn.metrics import silhouette_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "random.seed(1234)\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.linalg.norm(point1 - point2)\n",
        "\n",
        "def calculate_sse(clusters, centroids):\n",
        "    sse = 0\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        centroid = centroids[i]\n",
        "        for point in cluster:\n",
        "            sse += euclidean_distance(point, centroid) ** 2\n",
        "    return sse\n",
        "\n",
        "\n",
        "'''def calculate_sse(clusters, centroids):\n",
        "    sse = 0\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        centroid = centroids[i]\n",
        "        for point in cluster:\n",
        "            sse += euclidean_distance(point, centroid) ** 2\n",
        "        # Penalize for overlap\n",
        "        for j, other_cluster in enumerate(clusters):\n",
        "            if i != j:\n",
        "                for point in other_cluster:\n",
        "                    sse += euclidean_distance(point, centroid) ** 2\n",
        "    return sse'''\n",
        "\n",
        "def calculate_sse_single_cluster(clusters, centroid):\n",
        "    sse = 0\n",
        "    for point in cluster:\n",
        "            sse += euclidean_distance(point, centroid) ** 2\n",
        "    return sse\n",
        "\n",
        "def initialize_clusters(data, k):\n",
        "    n = len(data)\n",
        "    indices = np.random.choice(n, k, replace=False)\n",
        "    centroids = data[indices]\n",
        "    clusters = [[] for _ in range(k)]\n",
        "    for point in data:\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        nearest_centroid = np.argmin(distances)\n",
        "        clusters[nearest_centroid].append(point)\n",
        "    return clusters, centroids\n",
        "\n",
        "\n",
        "def is_feasible_move(data_point, current_cluster, new_cluster,k):\n",
        "    return len(new_cluster) < k and data_point not in new_cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methods**"
      ],
      "metadata": {
        "id": "2LkY2kG3JhDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Population based metaheuristics:\n",
        "\n",
        "\n",
        "*   Genetic Algorithm\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ojh-1fWGsu4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# centriod of clusters\n",
        "def get_centroids(clusters):\n",
        "   centroids = []\n",
        "   for cluster in clusters:\n",
        "      centroid = np.mean(cluster, axis=0)\n",
        "      centroids.append(centroid)\n",
        "   return centroids\n",
        "\n",
        "# clustering initialization\n",
        "def initialize_clusters(data, k):\n",
        "    n = len(data)\n",
        "    indices = np.random.choice(n, k, replace=False)\n",
        "    centroids = data[indices]\n",
        "    clusters = [[] for _ in range(k)]\n",
        "\n",
        "    for point in data:\n",
        "        distances = [euclidean_distance(point, centroid)\n",
        "                       for centroid in centroids]\n",
        "        nearest_centroid = np.argmin(distances)\n",
        "        clusters[nearest_centroid].append(point)\n",
        "\n",
        "    return clusters, centroids\n",
        "\n",
        "# distance calculation\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.linalg.norm(point1 - point2)\n",
        "\n",
        "# calculate_sse\n",
        "def calculate_sse(clusters, centroids):\n",
        "    sse = 0\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        centroid = centroids[i]\n",
        "        for point in cluster:\n",
        "            sse += euclidean_distance(point, centroid)**2\n",
        "    return sse\n",
        "\n",
        "# Decode chromosome to clusters\n",
        "def decode(solution):\n",
        "   clusters = [[] for _ in range(num_clusters)]\n",
        "   for i, cluster in enumerate(solution):\n",
        "       clusters[cluster].append(X[i])\n",
        "   return clusters\n",
        "\n",
        "\n",
        "# Fitness function\n",
        "def fitness_function(solution):\n",
        "   clusters = decode(solution)\n",
        "   centroids = get_centroids(clusters)\n",
        "   sse = calculate_sse(clusters, centroids)\n",
        "   return 1/sse\n",
        "\n",
        "\n",
        "# population initialization\n",
        "def initialize_population():\n",
        "   population = []\n",
        "   for _ in range(population_size):\n",
        "       solution = np.random.choice([0, 1], size=X.shape[1], replace=True)\n",
        "       population.append(solution)\n",
        "\n",
        "   return population\n",
        "\n",
        "# crossover\n",
        "def crossover(parent1, parent2):\n",
        "   crossover_point = np.random.randint(1, len(parent1) - 1)\n",
        "   child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "   child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "   return child1, child2\n",
        "\n",
        "# mutation\n",
        "def mutate(solution):\n",
        "   mutated_solution = np.copy(solution)\n",
        "   for i in range(len(solution)):\n",
        "       if np.random.random() < mutation_rate:\n",
        "           mutated_solution[i] = 1 - mutated_solution[i]\n",
        "   return mutated_solution\n",
        "\n",
        "# selection\n",
        "def select_parents(population, fitness_values):\n",
        "   total_fitness = np.sum(fitness_values)\n",
        "   probabilities = fitness_values / total_fitness\n",
        "   selected_indices = np.random.choice(range(len(population)), size=2, replace=False, p=probabilities)\n",
        "   return population[selected_indices[0]], population[selected_indices[1]]\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_population(population):\n",
        "   fitness_values = []\n",
        "   for solution in population:\n",
        "       fitness = fitness_function(solution)\n",
        "       fitness_values.append(fitness)\n",
        "   return fitness_values\n",
        "\n",
        "# Main GA loop\n",
        "def ga_clustering():\n",
        "\n",
        "   start_time = time.time()\n",
        "\n",
        "   population = initialize_population()\n",
        "   for _ in range(population_size):\n",
        "      fitness_values = evaluate_population(population)\n",
        "      new_population = []\n",
        "\n",
        "      for _ in range(population_size // 2):\n",
        "          parent1, parent2 = select_parents(population, fitness_values)\n",
        "          child1, child2 = crossover(parent1, parent2)\n",
        "          mutated_child1 = mutate(child1)\n",
        "          mutated_child2 = mutate(child2)\n",
        "          new_population.extend([mutated_child1, mutated_child2])\n",
        "\n",
        "      population = new_population\n",
        "\n",
        "   best_solution = population[np.argmax(fitness_values)]\n",
        "   end_time = time.time()\n",
        "   execution_time = end_time - start_time\n",
        "\n",
        "   # Evaluate best solution\n",
        "   clusters = decode(best_solution)\n",
        "   centroids = get_centroids(clusters)\n",
        "   sse = calculate_sse(clusters, centroids)\n",
        "\n",
        "   # to compute silhouette_score we have to get labels based on clusters\n",
        "   labels = [-1] * len(X)\n",
        "   for i, cluster in enumerate(clusters):\n",
        "      for j, point in enumerate(cluster):\n",
        "        labels[j] = i\n",
        "\n",
        "   silhouette = silhouette_score(X, labels)\n",
        "\n",
        "   return best_solution, sse, silhouette,execution_time"
      ],
      "metadata": {
        "id": "6OjnhqaptLtR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okvLoQa5s0Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfRhpPDmLrD_"
      },
      "outputs": [],
      "source": [
        "result=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLvzCqKbDyXi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_loader(choice, cases):\n",
        "\n",
        "    data = num_cluster = 0\n",
        "    match choice:\n",
        "        case 1:\n",
        "\n",
        "            data = load_iris().data\n",
        "            num_cluster = len(set(load_iris().target))\n",
        "\n",
        "        case 2:\n",
        "\n",
        "            data = load_wine().data\n",
        "            num_cluster = len(set(load_wine().target))\n",
        "\n",
        "        case 3:\n",
        "\n",
        "            file_ = pd.read_excel('concrete_data.xlsx')\n",
        "            data = file_.iloc[:, :-1].values\n",
        "            num_cluster = 7\n",
        "\n",
        "        case 4:\n",
        "\n",
        "            file_= pd.read_csv('breast_cancer_data.csv')\n",
        "            data = file_.drop(columns=['diagnosis']).values\n",
        "            num_cluster = file_['diagnosis'].nunique()\n",
        "\n",
        "        case _:\n",
        "            print(\"no data has been choosen\")\n",
        "    return data, num_cluster\n",
        "\n",
        "\n",
        "datasets_names={1: \"iris_data\", 2: \"wine_data\", 3: \"concrete_data\", 4: \"breast_cancer_data\" }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hhX0ISuiGx9e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "population_size_list = [30,50,100]\n",
        "\n",
        "mutation_rate_list = [0.001,0.01,0.1]\n",
        "\n",
        "\n",
        "Data_choice = [1,2,3,4]\n",
        "\n",
        "number_of_runs = 10\n",
        "sse = execution_time = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(number_of_runs):\n",
        "  for j in Data_choice:\n",
        "    for p in population_size_list:\n",
        "      for m in mutation_rate_list:\n",
        "        dataset_name = datasets_names.get(j)\n",
        "        X , num_clusters = data_loader(j, datasets_names)\n",
        "        population_size = p\n",
        "        mutation_rate = m\n",
        "\n",
        "        best_solution, sse, silhouette,execution_time = ga_clustering()\n",
        "\n",
        "        result.append({\n",
        "                    \"dataset\": dataset_name,\n",
        "                    \"algorithm\": 'GA',\n",
        "                    \"best_sse\": sse,\n",
        "                    \"execution_time\": execution_time,\n",
        "                    \"silhouette_avg\": silhouette,\n",
        "                    \"population_size\":population_size,\n",
        "                    \"mutation_rate\": mutation_rate,\n",
        "                    \"run_no\": i\n",
        "                })\n",
        "\n",
        "      #print(\"Done with Dataset\", dataset_name)\n",
        "  print(\"Done with run no:\", i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OD-BJBsvkHg",
        "outputId": "5d9176cb-8ece-46a4-ca4c-d0705abbb40a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done with run no: 0\n",
            "Done with run no: 1\n",
            "Done with run no: 2\n",
            "Done with run no: 3\n",
            "Done with run no: 4\n",
            "Done with run no: 5\n",
            "Done with run no: 6\n",
            "Done with run no: 7\n",
            "Done with run no: 8\n",
            "Done with run no: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#result"
      ],
      "metadata": {
        "id": "Q4UuVb3H39Ld"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "current_datetime = datetime.now()\n",
        "filename = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".xlsx\"\n",
        "final_result = pd.DataFrame(result)\n",
        "final_result.to_excel('result_'+filename)"
      ],
      "metadata": {
        "id": "29DyOSs70-Cb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
