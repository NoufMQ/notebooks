{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoufMQ/notebooks/blob/main/single_metahurestics_v0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cunn7iZxdAw",
        "outputId": "5f2ed5fc-dc7e-43a2-abc5-ae7e78c51493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOTMzoRBdiyv"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/cs_668')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Fsh_IKVi_p",
        "outputId": "8cb1cf4e-dbfe-4754-e929-75638013a64a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/cs_668\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xca39p_Gxeax"
      },
      "source": [
        "Dataets:\n",
        "  * From LR : iris , from sklearn.datasets\n",
        "  * From LR :  wine,   from sklearn.datasets\n",
        "  * From LR: Concrete Compressive Strength\n",
        " https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength , uploaded in drive\n",
        "\n",
        "  * From LR : Breast Cancer Wisconsin : https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/data , uploaded in drive\n",
        "\n",
        "  * covertype : http://archive.ics.uci.edu/dataset/31/covertype , uploaded in drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SENA2fnQX81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris,load_wine\n",
        "import random\n",
        "from collections import deque\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import math\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "random.seed(1234)\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.linalg.norm(point1 - point2)\n",
        "\n",
        "def calculate_sse(clusters, centroids):\n",
        "    sse = 0\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        centroid = centroids[i]\n",
        "        for point in cluster:\n",
        "            sse += euclidean_distance(point, centroid) ** 2\n",
        "    return sse\n",
        "\n",
        "\n",
        "'''def calculate_sse(clusters, centroids):\n",
        "    sse = 0\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        centroid = centroids[i]\n",
        "        for point in cluster:\n",
        "            sse += euclidean_distance(point, centroid) ** 2\n",
        "        # Penalize for overlap\n",
        "        for j, other_cluster in enumerate(clusters):\n",
        "            if i != j:\n",
        "                for point in other_cluster:\n",
        "                    sse += euclidean_distance(point, centroid) ** 2\n",
        "    return sse'''\n",
        "\n",
        "def calculate_sse_single_cluster(clusters, centroid):\n",
        "    sse = 0\n",
        "    for point in cluster:\n",
        "            sse += euclidean_distance(point, centroid) ** 2\n",
        "    return sse\n",
        "\n",
        "def initialize_clusters(data, k):\n",
        "    n = len(data)\n",
        "    indices = np.random.choice(n, k, replace=False)\n",
        "    centroids = data[indices]\n",
        "    clusters = [[] for _ in range(k)]\n",
        "    for point in data:\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        nearest_centroid = np.argmin(distances)\n",
        "        clusters[nearest_centroid].append(point)\n",
        "    return clusters, centroids\n",
        "\n",
        "\n",
        "def is_feasible_move(data_point, current_cluster, new_cluster,k):\n",
        "    return len(new_cluster) < k and data_point not in new_cluster\n",
        "\n",
        "def perturb(clusters):\n",
        "    data_point1_index = np.random.randint(len(clusters[0]))\n",
        "    data_point2_index = np.random.randint(len(clusters[1]))\n",
        "\n",
        "    data_point1 = clusters[0][data_point1_index]\n",
        "    data_point2 = clusters[1][data_point2_index]\n",
        "\n",
        "    new_clusters = clusters.copy()\n",
        "    new_clusters[0][data_point1_index] = data_point2\n",
        "    new_clusters[1][data_point2_index] = data_point1\n",
        "\n",
        "    return new_clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methods**"
      ],
      "metadata": {
        "id": "2LkY2kG3JhDx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxagOfSf_ImJ"
      },
      "outputs": [],
      "source": [
        "#for ILS\n",
        "def local_search(clusters, centroids,k):\n",
        "    sse = calculate_sse(clusters, centroids)\n",
        "    improved = True\n",
        "\n",
        "    while improved:\n",
        "        improved = False\n",
        "        for i, cluster in enumerate(clusters):\n",
        "            for j in range(len(cluster)):\n",
        "                data_point = cluster[j]\n",
        "                for new_cluster_index in range(k):\n",
        "                    if new_cluster_index != i and is_feasible_move(data_point, i, clusters[new_cluster_index],k):\n",
        "                        new_clusters = clusters.copy()\n",
        "                        new_centroids = centroids.copy()\n",
        "\n",
        "                        new_clusters[i].remove(data_point)\n",
        "\n",
        "                        new_clusters[new_cluster_index].append(data_point)\n",
        "\n",
        "                        new_centroids[i] = np.mean(new_clusters[i], axis=0) if len(new_clusters[i]) > 0 else np.zeros(len(centroids[0]))\n",
        "                        new_centroids[new_cluster_index] = np.mean(new_clusters[new_cluster_index], axis=0) if len(new_clusters[new_cluster_index]) > 0 else np.zeros(len(centroids[0]))\n",
        "\n",
        "                        new_sse = calculate_sse(new_clusters, new_centroids)\n",
        "\n",
        "                        if new_sse < sse:\n",
        "                            clusters = new_clusters.copy()\n",
        "                            centroids = new_centroids.copy()\n",
        "                            sse = new_sse\n",
        "                            improved = True\n",
        "                            break\n",
        "\n",
        "    return clusters, centroids\n",
        "#ILS\n",
        "def iterated_local_search(data, k, max_iterations):\n",
        "\n",
        "    start_time = time.time()\n",
        "    silhouette_avg = -2\n",
        "    n = len(data)\n",
        "    dimensions = len(data[0])\n",
        "    best_clusters, best_centroids = initialize_clusters(data, k)\n",
        "    best_sse = calculate_sse(best_clusters, best_centroids)\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        current_clusters, current_centroids = best_clusters.copy(), best_centroids.copy()\n",
        "        current_sse = best_sse\n",
        "\n",
        "        current_clusters = perturb(current_clusters)\n",
        "\n",
        "        current_clusters, current_centroids = local_search(current_clusters, current_centroids,k)\n",
        "        current_sse = calculate_sse(current_clusters, current_centroids)\n",
        "\n",
        "        if current_sse < best_sse:\n",
        "            best_clusters = current_clusters.copy()\n",
        "            best_centroids = current_centroids.copy()\n",
        "            best_sse = current_sse\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    transformed_array = [[subarray_value, array_index] for array_index, subarray in enumerate(best_clusters) for subarray_value in subarray]\n",
        "    first_column_values = [subarray_value for subarray_value, array_index in transformed_array]\n",
        "    second_column_indices = [array_index for subarray, array_index in transformed_array]\n",
        "\n",
        "    silhouette_avg = silhouette_score(first_column_values,second_column_indices)\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "\n",
        "    return best_sse, execution_time , silhouette_avg\n",
        "\n",
        "#SA\n",
        "def simulated_annealing_clustering(X, k,initial_temp, cooling_rate, num_iterations):\n",
        "    start_time = time.time()\n",
        "    silhouette_avg =-2\n",
        "    n_clusters = k\n",
        "\n",
        "    crnt_sol, crnt_centroids = initialize_clusters(X, n_clusters)\n",
        "    crnt_cost = calculate_sse(crnt_sol, crnt_centroids)\n",
        "\n",
        "    best_solution = crnt_sol\n",
        "    best_centroids = crnt_centroids\n",
        "    best_cost = crnt_cost\n",
        "\n",
        "    temperature = initial_temp\n",
        "\n",
        "    best_costs = []\n",
        "    best_sse = 0\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        new_sol, new_centroids = initialize_clusters(X, n_clusters)\n",
        "        new_cost = calculate_sse(new_sol, new_centroids)\n",
        "\n",
        "        if new_cost < crnt_cost:\n",
        "            crnt_sol = new_sol\n",
        "            crnt_centroids = new_centroids\n",
        "            crnt_cost = new_cost\n",
        "        else:\n",
        "            if np.random.rand() < np.exp((crnt_cost - new_cost) / temperature):\n",
        "                crnt_sol = new_sol\n",
        "                crnt_centroids = new_centroids\n",
        "                crnt_cost = new_cost\n",
        "\n",
        "        if crnt_cost < best_cost:\n",
        "            best_solution = crnt_sol\n",
        "            best_centroids = crnt_centroids\n",
        "            best_cost = crnt_cost\n",
        "            best_sse = crnt_cost\n",
        "        temperature *= cooling_rate\n",
        "\n",
        "        best_costs.append(best_cost)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    transformed_array = [[subarray_value, array_index] for array_index, subarray in enumerate(best_solution) for subarray_value in subarray]\n",
        "    first_column_values = [subarray_value for subarray_value, array_index in transformed_array]\n",
        "    second_column_indices = [array_index for subarray, array_index in transformed_array]\n",
        "\n",
        "    silhouette_avg = silhouette_score(first_column_values,second_column_indices)\n",
        "\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    return best_sse,execution_time, silhouette_avg\n",
        "\n",
        "#classic LS\n",
        "def hill_climbing(data, k, max_iterations):\n",
        "    start_time = time.time()\n",
        "    silhouette_avg = -2\n",
        "    n = len(data)\n",
        "    dimensions = len(data[0])\n",
        "\n",
        "    best_clusters, best_centroids = initialize_clusters(data, k)\n",
        "    best_sse = calculate_sse(best_clusters, best_centroids)\n",
        "\n",
        "    current_clusters = best_clusters.copy()\n",
        "    current_centroids = best_centroids.copy()\n",
        "    current_sse = best_sse\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        data_point_index = np.random.randint(n)\n",
        "        current_cluster = next((i for i, cluster in enumerate(current_clusters) if np.array_equal(data[data_point_index], cluster)), None)\n",
        "\n",
        "        if current_cluster is None:\n",
        "            continue\n",
        "\n",
        "\n",
        "        for i in range(k):\n",
        "            if i != current_cluster:\n",
        "                if current_cluster is not None:\n",
        "                    current_clusters[current_cluster].remove(data[data_point_index])\n",
        "\n",
        "                new_clusters = current_clusters.copy()\n",
        "                new_centroids = current_centroids.copy()\n",
        "\n",
        "                new_clusters[i].append(data[data_point_index])\n",
        "\n",
        "                new_centroids[current_cluster] = np.mean(new_clusters[current_cluster], axis=0)\n",
        "                new_centroids[i] = np.mean(new_clusters[i], axis=0)\n",
        "\n",
        "                new_sse = calculate_sse(new_clusters, new_centroids)\n",
        "\n",
        "                if new_sse < current_sse:\n",
        "                    current_clusters = new_clusters.copy()\n",
        "                    current_centroids = new_centroids.copy()\n",
        "                    current_sse = new_sse\n",
        "\n",
        "        if current_sse < best_sse:\n",
        "            best_clusters = current_clusters.copy()\n",
        "            best_centroids = current_centroids.copy()\n",
        "            best_sse = current_sse\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "\n",
        "    transformed_array = [[subarray_value, array_index] for array_index, subarray in enumerate(best_clusters) for subarray_value in subarray]\n",
        "    first_column_values = [subarray_value for subarray_value, array_index in transformed_array]\n",
        "    second_column_indices = [array_index for subarray, array_index in transformed_array]\n",
        "\n",
        "    silhouette_avg = silhouette_score(first_column_values,second_column_indices)\n",
        "\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    return best_sse, execution_time, silhouette_avg\n",
        "\n",
        "#TS\n",
        "def tabu_search(data, k, max_iterations, tabu_size):\n",
        "    start_time = time.time()\n",
        "    silhouette_avg = -2\n",
        "    n = len(data)\n",
        "    dimensions = len(data[0])\n",
        "    best_clusters, best_centroids = initialize_clusters(data, k)\n",
        "    best_sse = calculate_sse(best_clusters, best_centroids)\n",
        "    current_clusters = best_clusters.copy()\n",
        "    current_centroids = best_centroids.copy()\n",
        "    current_sse = best_sse\n",
        "\n",
        "    tabu_list = deque(maxlen=tabu_size)\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        data_point_index = np.random.randint(n)\n",
        "        current_cluster = next((i for i, cluster in enumerate(current_clusters) if np.array_equal(data[data_point_index], cluster)), None)\n",
        "\n",
        "        if current_cluster is None:\n",
        "            continue\n",
        "\n",
        "        possible_moves = []\n",
        "        for i in range(k):\n",
        "            if i != current_cluster:\n",
        "                new_cluster = current_clusters[i].copy()\n",
        "\n",
        "                if is_feasible_move(data[data_point_index], current_cluster, new_cluster,k):\n",
        "                    new_clusters = current_clusters.copy()\n",
        "                    new_centroids = current_centroids.copy()\n",
        "\n",
        "                    new_clusters[i].append(data[data_point_index])\n",
        "\n",
        "                    if current_cluster is not None:\n",
        "                        new_clusters[current_cluster].remove(data[data_point_index])\n",
        "\n",
        "                    new_centroids[current_cluster] = np.mean(new_clusters[current_cluster], axis=0)\n",
        "                    new_centroids[i] = np.mean(new_clusters[i], axis=0)\n",
        "\n",
        "                    new_sse = calculate_sse(new_clusters, new_centroids)\n",
        "\n",
        "                    possible_moves.append((new_clusters, new_centroids, new_sse))\n",
        "\n",
        "        best_move = None\n",
        "        for move in possible_moves:\n",
        "            clusters, centroids, sse = move\n",
        "            if (clusters, centroids) not in tabu_list:\n",
        "                if best_move is None or sse < best_sse:\n",
        "                    best_move = move\n",
        "\n",
        "        if best_move is None:\n",
        "            continue\n",
        "\n",
        "        current_clusters, current_centroids, current_sse = best_move\n",
        "\n",
        "        tabu_list.append((current_clusters, current_centroids))\n",
        "\n",
        "        if current_sse < best_sse:\n",
        "            best_clusters = current_clusters.copy()\n",
        "            best_centroids = current_centroids.copy()\n",
        "            best_sse = current_sse\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    transformed_array = [[subarray_value, array_index] for array_index, subarray in enumerate(best_clusters) for subarray_value in subarray]\n",
        "    first_column_values = [subarray_value for subarray_value, array_index in transformed_array]\n",
        "    second_column_indices = [array_index for subarray, array_index in transformed_array]\n",
        "\n",
        "    silhouette_avg = silhouette_score(first_column_values,second_column_indices)\n",
        "\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "\n",
        "    return best_sse, execution_time,silhouette_avg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLvzCqKbDyXi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_loader(choice, cases):\n",
        "\n",
        "    data = num_cluster = 0\n",
        "    match choice:\n",
        "        case 1:\n",
        "\n",
        "            data = load_iris().data\n",
        "            num_cluster = len(set(load_iris().target))\n",
        "\n",
        "        case 2:\n",
        "\n",
        "            data = load_wine().data\n",
        "            num_cluster = len(set(load_wine().target))\n",
        "\n",
        "        case 3:\n",
        "\n",
        "            file_ = pd.read_excel('concrete_data.xlsx')\n",
        "            data = file_.iloc[:, :-1].values\n",
        "            num_cluster = 7\n",
        "\n",
        "        case 4:\n",
        "\n",
        "            file_= pd.read_csv('breast_cancer_data.csv')\n",
        "            data = file_.drop(columns=['diagnosis']).values\n",
        "            num_cluster = file_['diagnosis'].nunique()\n",
        "\n",
        "        case 5:\n",
        "            file_= pd.read_csv('covertype_csv.csv')\n",
        "            data = file_.iloc[:, :-1].values\n",
        "            num_cluster = file_['class'].nunique()\n",
        "\n",
        "        case _:\n",
        "            print(\"no data has been choosen\")\n",
        "    return data, num_cluster\n",
        "\n",
        "\n",
        "datasets_names={1: \"iris_data\", 2: \"wine_data\", 3: \"concrete_data\", 4: \"breast_cancer_data\" , 5: \"covertype_data\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfRhpPDmLrD_"
      },
      "outputs": [],
      "source": [
        "result=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhX0ISuiGx9e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "Data_choice = [1,2,3,4] #can change from 1,2,3,4,5 for datasets\n",
        "max_iterations = [10, 100,1000] #can change from 10,100,1000,10000 Return to the LR to explore other number of iterations\n",
        "tabu_tenure = [0.05, 0.1, 0.15 , 0.20, 0.25, 0.5]\n",
        "number_of_runs = 10\n",
        "best_sse = excution_time = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(number_of_runs):\n",
        "  for j in Data_choice:\n",
        "      dataset_name = datasets_names.get(j)\n",
        "      data , num_classes = data_loader(j, datasets_names)\n",
        "      for max_i in max_iterations:\n",
        "\n",
        "        best_sse, excution_time, silhouette_avg= hill_climbing(data, num_classes, max_i)\n",
        "        result.append({\n",
        "                    \"dataset\": dataset_name,\n",
        "                    \"algorithm\": 'hill_climbing',\n",
        "                    \"best_sse\": best_sse,\n",
        "                    \"execution_time\": excution_time,\n",
        "                    \"max_iterations\":max_i,\n",
        "                    \"silhouette_avg\": silhouette_avg,\n",
        "                    \"run_no\": i,\n",
        "                    \"parameter\": 0\n",
        "                })\n",
        "\n",
        "\n",
        "        initial_temp = 100 #this need to be tuned\n",
        "        cooling_rate = 0.95\n",
        "        best_sse, excution_time, silhouette_avg= simulated_annealing_clustering(data,num_classes, initial_temp, cooling_rate, max_i)\n",
        "        result.append({\n",
        "                    \"dataset\": dataset_name,\n",
        "                    \"algorithm\": 'simulated_annealing_clustering',\n",
        "                    \"best_sse\": best_sse,\n",
        "                    \"execution_time\": excution_time,\n",
        "                    \"max_iterations\":max_i,\n",
        "                    \"silhouette_avg\": silhouette_avg,\n",
        "                    \"run_no\": i,\n",
        "                    \"parameter\": [initial_temp,cooling_rate]\n",
        "                })\n",
        "\n",
        "        for t in tabu_tenure:\n",
        "          best_sse, excution_time, silhouette_avg = tabu_search(data, num_classes, max_i, tabu_size=math.ceil((len(data) * t)))\n",
        "          result.append({\n",
        "                      \"dataset\": dataset_name,\n",
        "                      \"algorithm\": 'tabu_search',\n",
        "                      \"best_sse\": best_sse,\n",
        "                      \"execution_time\": excution_time,\n",
        "                      \"silhouette_avg\": silhouette_avg,\n",
        "                      \"max_iterations\":max_i,\n",
        "                      \"run_no\": i,\n",
        "                      \"parameter\": [t, math.ceil((len(data) * t))]\n",
        "                  })\n",
        "        #print(\"Done with tabu_search for: \",dataset_name, \" For run: \", i)\n",
        "\n",
        "\n",
        "        best_sse, excution_time, silhouette_avg = iterated_local_search(data, num_classes, max_i)\n",
        "        result.append({\n",
        "                    \"dataset\": dataset_name,\n",
        "                    \"algorithm\": 'iterated_local_search',\n",
        "                    \"best_sse\": best_sse,\n",
        "                    \"execution_time\": excution_time,\n",
        "                    \"silhouette_avg\": silhouette_avg,\n",
        "                    \"max_iterations\":max_i,\n",
        "                    \"run_no\": i,\n",
        "                    \"parameter\":0\n",
        "\n",
        "                })\n",
        "      print(\"Done with Dataset\", dataset_name)\n",
        "  print(\"Done with run no:\", i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OD-BJBsvkHg",
        "outputId": "d12c0f17-6f99-4c6e-f654-29cfa830f98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done with Dataset iris_data\n",
            "Done with Dataset wine_data\n",
            "Done with Dataset concrete_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "Q4UuVb3H39Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "current_datetime = datetime.now()\n",
        "filename = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".xlsx\"\n",
        "final_result = pd.DataFrame(result)\n",
        "final_result.to_excel('result_'+filename)"
      ],
      "metadata": {
        "id": "29DyOSs70-Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EEthMxCQv1C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db4KAprAQv3Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}